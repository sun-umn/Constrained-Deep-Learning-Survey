{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a4258ce-b448-4e6d-8b74-fe2ed91bed68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ryandevera/data-science/umn_environments/Constrained-Deep-Learning-Survey/notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47dd756d-d2a4-4592-ba35-95d46d03c5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ryandevera/data-science/umn_environments/Constrained-Deep-Learning-Survey\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryandevera/.virtualenvs/cdl-survey/lib/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1956e81c-2397-40d8-b7ec-d6dfad14dbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-23 11:52:30.575163: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# stdlib\n",
    "import math\n",
    "import random\n",
    "\n",
    "# third party\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow_constrained_optimization as tfco\n",
    "import warnings\n",
    "\n",
    "# first party\n",
    "from cdlsurvey.data import get_data\n",
    "from cdlsurvey.metrics import get_exp_error_rate_constraints\n",
    "from cdlsurvey.models import Model\n",
    "from cdlsurvey.utils import training_helper\n",
    "\n",
    "# Disable eager execution\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "# suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For plotting in notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0898f66b-cfbf-42b6-a8a2-a709bb306737",
   "metadata": {},
   "source": [
    "# Reading and processing dataset\n",
    "\n",
    "We load the [UCI Adult Dataset](https://archive.ics.uci.edu/dataset/2/adult) and we do some pre-processing. The way this was done in the `TFCO` tutorial is by:\n",
    "\n",
    "-  Binarizing the categorial variables\n",
    "-  Binning and binarizing the continuous variables.\n",
    "\n",
    "The dataset is based on census data and the goal is to predict whether someone's income is over 50k. We construct four protected groups, two based on gender (Male and Female) and two based on race (White and Black). The pre-processing of the features based on the works [Fairness Constraints: Mechanisms for Fair Classification](https://arxiv.org/pdf/1507.05259.pdf) and [Satisfying Real-World Goals with Dataset Constraints](https://arxiv.org/pdf/1606.07558.pdf).\n",
    "\n",
    "The fairness goal is that of equal opportunity. That is, we would like the **true positive rates** of our classifier on the protected groups to match that of the overall dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0181c8b-6878-4ff4-bcb7-eb73585692bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_COLUMNS = [\n",
    "    'workclass', 'education', 'marital_status', 'occupation', 'relationship',\n",
    "    'race', 'gender', 'native_country'\n",
    "]\n",
    "CONTINUOUS_COLUMNS = [\n",
    "    'age', 'capital_gain', 'capital_loss', 'hours_per_week', 'education_num'\n",
    "]\n",
    "COLUMNS = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
    "    'marital_status', 'occupation', 'relationship', 'race', 'gender',\n",
    "    'capital_gain', 'capital_loss', 'hours_per_week', 'native_country',\n",
    "    'income_bracket'\n",
    "]\n",
    "LABEL_COLUMN = 'label'\n",
    "\n",
    "PROTECTED_COLUMNS = [\n",
    "    'gender_Female', 'gender_Male', 'race_White', 'race_Black'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9715758-8e8e-4aea-9090-71ddd40a1f0e",
   "metadata": {},
   "source": [
    "# Data Function & Gathering\n",
    "\n",
    "The cells below will load in the Adult dataset and featureize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6367c64-5261-4b32-acf6-08f83e49cab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather the data\n",
    "train_df, test_df, FEATURE_NAMES = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c780842-7bdd-4e8a-b6b2-f12efc88a107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7841, 3846)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure there are positive labels\n",
    "train_df['label'].sum(), test_df['label'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f91201-500f-4122-a445-0601908b96db",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "We will use a simple 2-layer MLP and predict positively or negatively based on threshold at 0.\n",
    "\n",
    "The model code can be found here [Fairness MLP Model - Sun Group](https://github.com/sun-umn/Constrained-Deep-Learning-Survey/blob/main/cdlsurvey/models.py). In the code, we initialize the placeholders and model. In the `build_train_op`, we set up the constrained optimization problem. We create a rate context for the entire dataset, and compue the overall false positive rate as teh positive prediction rate on the negatively labeled subset.\n",
    "\n",
    "We then can construct a constraint for each of the protected groups based on the difference between the true positive rates of the protected groups and that of the overall dataset. We then construct a minimization problem using `RateMinimizationProblem` and use the `ProxyLagrangianOptimizerV1` as the solver. `build_train_op` initializes a training operation which will later be used to actually train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9ea387-4771-4425-8594-fa8e36ef7c0b",
   "metadata": {},
   "source": [
    "# Baseline without constraints\n",
    "\n",
    "We now declare the model, build the training op, and then perform the training. We use a 2-layer MLP, and train using the Adam optimizer with learning rate 0.01, with minibatch size of 100 over 40 epochs. We train without fairness constraints to show the baseline performance. We see that without training for fairness, we obtain a high fairness evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62c6ff35-0de3-4163-afb7-aa4a525d494d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(\n",
    "    tpr_max_diff=0.05,\n",
    "    protected_columns=PROTECTED_COLUMNS,\n",
    "    feature_names=FEATURE_NAMES,\n",
    "    label_column=LABEL_COLUMN,\n",
    ")\n",
    "model.build_train_op(0.01, unconstrained=True)\n",
    "\n",
    "# training_helper returns the list of errors and violations over each epoch.\n",
    "train_errors, train_violations, test_errors, test_violations = training_helper(\n",
    "    model,\n",
    "    train_df,\n",
    "    test_df,\n",
    "    100,\n",
    "    num_iterations_per_loop=326,\n",
    "    num_loops=40,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e7a518c-fe36-4a6b-9136-9f0c49f5d529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error 0.12118792420380209\n",
      "Train Violation -0.04409253750329134\n",
      "\n",
      "Test Error 0.14532276887169093\n",
      "Test Violation 0.02092328385872866\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Error\", train_errors[-1])\n",
    "print(\"Train Violation\", max(train_violations[-1]))\n",
    "print()\n",
    "print(\"Test Error\", test_errors[-1])\n",
    "print(\"Test Violation\", max(test_violations[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea186b3-6dcb-434e-94c6-bdaf0c15104a",
   "metadata": {},
   "source": [
    "# Training with fairness constraint\n",
    "\n",
    "We now show train with the constraints using the procedure of [Two-Player Games for Efficient Non=convex Constrained Optimization](https://arxiv.org/abs/1804.06500) and returning the last solution found. We see that the fairness violation improves.\n",
    "\n",
    "We allow an additive fairness slack of 0.05. That is, when training and evaluating the fairness constraints, the true postive rate difference between protected groups has to be at least that of the overall dataset up to a slack of at most 0.05. Thus, the fairness constraints woul dbe of the form $TPR_p \\geq TPR - 0.05$, where $TPR_p$ and TPR denotes the true postive rates of the protected group and the overall dataset, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "054ae740-2fa0-41b0-88d9-2f1d2280203d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(\n",
    "    tpr_max_diff=0.05,\n",
    "    protected_columns=PROTECTED_COLUMNS,\n",
    "    feature_names=FEATURE_NAMES,\n",
    "    label_column=LABEL_COLUMN,\n",
    ")\n",
    "model.build_train_op(0.01, unconstrained=False)\n",
    "\n",
    "# training_helper returns the list of errors and violations over each epoch.\n",
    "train_errors, train_violations, test_errors, test_violations = training_helper(\n",
    "      model,\n",
    "      train_df,\n",
    "      test_df,\n",
    "      100,\n",
    "      num_iterations_per_loop=326,\n",
    "      num_loops=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e55c114-91e1-4639-a4fd-d1843574b7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error 0.12192500230336906\n",
      "Train Violation -0.010769920490356633\n",
      "\n",
      "Test Error 0.14581413918064\n",
      "Test Violation 0.031323699875369315\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Error\", train_errors[-1])\n",
    "print(\"Train Violation\", max(train_violations[-1]))\n",
    "print()\n",
    "print(\"Test Error\", test_errors[-1])\n",
    "print(\"Test Violation\", max(test_violations[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7689ccc-aaa1-41f2-abea-6849b04512a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error 0.13148859064525045\n",
      "Train Violation -0.012193528525327266\n",
      "\n",
      "Test Error 0.1450049137030895\n",
      "Test Violation 0.017228812057510222\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Error\", np.mean(train_errors))\n",
    "print(\"Train Violation\", max(np.mean(train_violations, axis=0)))\n",
    "print()\n",
    "print(\"Test Error\", np.mean(test_errors))\n",
    "print(\"Test Violation\", max(np.mean(test_violations, axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2c428a-d3ca-4d4a-ad74-e824f801f613",
   "metadata": {},
   "source": [
    "# Improve using best iterate instead of last iterate\n",
    "\n",
    "As discussed in [Optimization with Non-differentiable Constraints with Applications to Fairness, Recall, Churn, and Other Goals](https://arxiv.org/abs/1809.04198), the last iterate may not be the best choice and suggests a simple heuristic to choose the best iterate out of the ones found after each epoch. The heuristic proceeds by ranking each of the solutions based on accuracy and fairness separately with respect to the training data. Any solutions which satisfy the constraints are equally ranked top in terms of fairness. Each solution thus has two ranks. Then, the solution chosen is the one with the smallest maximum of the two ranks. We see that this improves the fairness and can find a better accuracy / fairness trade-off on the training data.\n",
    "\n",
    "The solution can be calculated using `find_best_candidate_index` given the list of training errors and vioilations associated with each of the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8777d432-237f-4a46-a7c5-01e1aa162a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error 0.12192500230336906\n",
      "Train Violation -0.010769920490356633\n",
      "\n",
      "Test Error 0.14581413918064\n",
      "Test Violation 0.031323699875369315\n"
     ]
    }
   ],
   "source": [
    "best_cand_index = tfco.find_best_candidate_index(train_errors, train_violations)\n",
    "\n",
    "print(\"Train Error\", train_errors[best_cand_index])\n",
    "print(\"Train Violation\", max(train_violations[best_cand_index]))\n",
    "print()\n",
    "print(\"Test Error\", test_errors[best_cand_index])\n",
    "print(\"Test Violation\", max(test_violations[best_cand_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b52a073-e39b-470c-b03d-b4175fa4cedf",
   "metadata": {},
   "source": [
    "# Using stochastic solutions\n",
    "\n",
    "As discussed in [Two-Player Games for Efficient Non=convex Constrained Optimization](https://arxiv.org/abs/1804.06500), neither the best nor last iterate will come with theoretical guarantees. One can instead use randomlized solutions, which come with theoretical gurantees. However, as discussed in [Optimization with Non-differentiable Constraints with Applications to Fairness, Recall, Churn, and Other Goals](https://arxiv.org/abs/1809.04198), there may not always be a clear practical benefit. We show how to use these solutions here for sake of completeness.\n",
    "\n",
    "## T-stochastic solution\n",
    "\n",
    "The first and simplest randomized solution suggested is the T-stochastic, which simply takes the average of all the iterates found at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9979c6e2-2cfc-4920-8ce1-d40bedc14d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error 0.13148859064525045\n",
      "Train Violation -0.012193528525327266\n",
      "\n",
      "Test Error 0.1450049137030895\n",
      "Test Violation 0.017228812057510222\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Error\", np.mean(train_errors))\n",
    "print(\"Train Violation\", max(np.mean(train_violations, axis=0)))\n",
    "print()\n",
    "print(\"Test Error\", np.mean(test_errors))\n",
    "print(\"Test Violation\", max(np.mean(test_violations, axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c40cba-7ded-40ec-af28-af6639f53cf8",
   "metadata": {},
   "source": [
    "## M-stochastic solution\n",
    "\n",
    "[Two-Player Games for Efficient Non=convex Constrained Optimization](https://arxiv.org/abs/1804.06500) presents a method which shrinks down the **T-stochastic** solution down to the one that is supported on at most (m + 1) points where m is the number of constraints and is guaranteed to be at least as good as the T-stochastic solution. Here we see that indeed there is benefit in performing the shrinking.\n",
    "\n",
    "This solution can be computed using the `find_best_candidate_distribution` by passing in the training errors and violations found at each epoch and return the weight of each constituent. We see that indeed, it is sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fafbdc43-7347-4023-8012-20a6aed19b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "cand_dist = tfco.find_best_candidate_distribution(train_errors, train_violations)\n",
    "print(cand_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "974a0c3a-8322-49ac-9a97-624f69f4f333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error 0.12192500230336906\n",
      "Train Violation -0.010769920490356633\n",
      "\n",
      "Test Error 0.14581413918064\n",
      "Test Violation 0.031323699875369315\n"
     ]
    }
   ],
   "source": [
    "m_stoch_error_train, m_stoch_violations_train = get_exp_error_rate_constraints(cand_dist, train_errors, train_violations)\n",
    "m_stoch_error_test, m_stoch_violations_test = get_exp_error_rate_constraints(cand_dist, test_errors, test_violations)\n",
    "\n",
    "print(\"Train Error\", m_stoch_error_train)\n",
    "print(\"Train Violation\", max(m_stoch_violations_train))\n",
    "print()\n",
    "print(\"Test Error\", m_stoch_error_test)\n",
    "print(\"Test Violation\", max(m_stoch_violations_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aef882-1ae9-441a-a701-ffc127b3226b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdl-survey",
   "language": "python",
   "name": "cdl-survey"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
