{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29da5358-e50e-46f0-aab7-d60fb2415716",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d687b84a-0e0d-433c-9626-6b4bed95a54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1949c68-8796-4ce6-a5b1-1a77ab24967b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Load in datasets from ai360\n",
    "from aif360.datasets import MEPSDataset19\n",
    "from aif360.datasets import MEPSDataset20\n",
    "from aif360.datasets import MEPSDataset21\n",
    "\n",
    "# Load in the fairness metrics from ai360\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "# Load in the explainers\n",
    "from aif360.explainers import MetricTextExplainer\n",
    "\n",
    "# Scalers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load in bias mitigation techniques\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "from aif360.algorithms.inprocessing import PrejudiceRemover\n",
    "\n",
    "# Load in models from sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870a80c3-d056-4cfa-a632-ff961fb8f070",
   "metadata": {},
   "source": [
    "# Load data and create splits for training, validation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d952c76-cbfe-4fbc-99df-07d4e1892fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    dataset_orig_panel19_train,\n",
    "    dataset_orig_panel19_val,\n",
    "    dataset_orig_panel19_test\n",
    ") = MEPSDataset19().split([0.5, 0.8], shuffle=True)\n",
    "\n",
    "sens_ind = 0\n",
    "sens_attr = dataset_orig_panel19_train.protected_attribute_names[sens_ind]\n",
    "print(sens_attr)\n",
    "\n",
    "# Create unprivileged groups\n",
    "unprivileged_groups = [\n",
    "    {sens_attr: v} for v\n",
    "    in dataset_orig_panel19_train.unprivileged_protected_attributes[sens_ind]\n",
    "]\n",
    "print(unprivileged_groups)\n",
    "\n",
    "privileged_groups = [\n",
    "    {sens_attr: v} for v\n",
    "    in dataset_orig_panel19_train.privileged_protected_attributes[sens_ind]\n",
    "]\n",
    "print(privileged_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff09155d-4c58-4061-8e16-3d6790408360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is BinaryLabelDatasetMetric?\n",
    "metric_orig_panel19_train = BinaryLabelDatasetMetric(\n",
    "    dataset_orig_panel19_train,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups\n",
    ")\n",
    "\n",
    "# Setup an explainer\n",
    "# What is disparate impact?\n",
    "explainer_orig_panel19_train = MetricTextExplainer(metric_orig_panel19_train)\n",
    "print(explainer_orig_panel19_train.disparate_impact())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e734020a-122a-4707-97b5-717db137cbb6",
   "metadata": {},
   "source": [
    "# Learning a Logistic Regression Classifier on Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b999a64d-e3a4-490c-a042-0c23f6ca57e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our model\n",
    "from cdl_python.core.models import MLP\n",
    "\n",
    "# To use pytorch with sklearn lets use skorch\n",
    "import torch\n",
    "from skorch import NeuralNetBinaryClassifier\n",
    "\n",
    "dataset = dataset_orig_panel19_train\n",
    "\n",
    "mlp_model = MLP(num_features=dataset.features.shape[1], num_classes=1)\n",
    "mlp_model = mlp_model.to(dtype=torch.double)\n",
    "torch_model = NeuralNetBinaryClassifier(\n",
    "    mlp_model,\n",
    "    criterion=torch.nn.BCEWithLogitsLoss,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    lr=0.0001,\n",
    "    max_epochs=10,\n",
    "    batch_size=16,\n",
    ")\n",
    "\n",
    "# MLP from our cdl survey\n",
    "model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    torch_model\n",
    ")\n",
    "\n",
    "mlp_orig_panel19 = model.fit(dataset.features, dataset.labels.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41899a46-ec03-4413-8f87-4c2fdb1293b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def test(dataset, model, thresh_arr):\n",
    "    \"\"\"\n",
    "    Function to produce predictions and measures\n",
    "    for constrained deep learning survey\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # sklearn classifier\n",
    "        y_val_pred_prob = model.predict_proba(dataset.features)\n",
    "        pos_ind = np.where(np.array(model.classes_) == dataset.favorable_label)[0][0]\n",
    "\n",
    "    except AttributeError:\n",
    "        # aif360 inprocessing algorithm\n",
    "        y_val_pred_prob = model.predict(dataset).scores\n",
    "        pos_ind = 0\n",
    "\n",
    "    # Save the metrics\n",
    "    metric_arrs = defaultdict(list)\n",
    "    for thresh in thresh_arr:\n",
    "        # Get predictions and threshold to binaryu\n",
    "        y_val_pred = (y_val_pred_prob[:, pos_ind] > thresh).astype(np.float64)\n",
    "\n",
    "        # Build dataset with predictions\n",
    "        dataset_pred = dataset.copy()\n",
    "        dataset_pred.labels = y_val_pred\n",
    "\n",
    "        # Compute the metrics\n",
    "        metric = ClassificationMetric(\n",
    "            dataset,\n",
    "            dataset_pred,\n",
    "            unprivileged_groups=unprivileged_groups,\n",
    "            privileged_groups=privileged_groups,\n",
    "        )\n",
    "\n",
    "        # Balanced Accuracy\n",
    "        metric_arrs['bal_acc'].append(\n",
    "            (metric.true_positive_rate() + metric.true_negative_rate()) / 2\n",
    "        )\n",
    "        # Average odds diff\n",
    "        metric_arrs['avg_odds_diff'].append(metric.average_odds_difference())\n",
    "\n",
    "        # Disparity impact\n",
    "        metric_arrs['disp_imp'].append(metric.disparate_impact())\n",
    "\n",
    "        # Stat par diff - will need to look into the paper\n",
    "        metric_arrs['stat_par_diff'].append(metric.statistical_parity_difference())\n",
    "\n",
    "        # Equal opp diff - will need to look into the paper\n",
    "        metric_arrs['eq_opp_diff'].append(metric.equal_opportunity_difference())\n",
    "\n",
    "        # Not sure what this metric is will also need to look into the paper\n",
    "        metric_arrs['theil_ind'].append(metric.theil_index())\n",
    "    \n",
    "    return metric_arrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae5a32a-e3fd-4d6c-a9ed-ce529ebf6ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_arr = np.linspace(0.01, 0.5, 50)\n",
    "val_metrics = test(\n",
    "    dataset=dataset_orig_panel19_val,\n",
    "    model=lr_orig_panel19,\n",
    "    thresh_arr=thresh_arr\n",
    ")\n",
    "mlp_orig_best_ind = np.argmax(val_metrics['bal_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e6f832-5cf2-4a01-b49d-ac2612e30b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(x, x_name, y_left, y_left_name, y_right, y_right_name):\n",
    "    fig, ax1 = plt.subplots(figsize=(7, 4))\n",
    "    ax1.plot(x, y_left)\n",
    "    ax1.set_xlabel(x_name, fontsize=16, fontweight='bold')\n",
    "    ax1.set_ylabel(y_left_name, color='b', fontsize=16, fontweight='bold')\n",
    "    ax1.xaxis.set_tick_params(labelsize=14)\n",
    "    ax1.yaxis.set_tick_params(labelsize=14)\n",
    "    ax1.set_ylim(0.5, 0.8)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(x, y_right, color='r')\n",
    "    ax2.set_ylabel(y_right_name, color='r', fontsize=16, fontweight='bold')\n",
    "    if 'DI' in y_right_name:\n",
    "        ax2.set_ylim(0., 0.7)\n",
    "    else:\n",
    "        ax2.set_ylim(-0.25, 0.1)\n",
    "\n",
    "    best_ind = np.argmax(y_left)\n",
    "    ax2.axvline(np.array(x)[best_ind], color='k', linestyle=':')\n",
    "    ax2.yaxis.set_tick_params(labelsize=14)\n",
    "    ax2.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1df62bb-668c-4b3a-8cfc-d2e5311c4e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_imp = np.array(val_metrics['disp_imp'])\n",
    "disp_imp_err = 1 - np.minimum(disp_imp, 1 / disp_imp)\n",
    "plot(\n",
    "    thresh_arr,\n",
    "    'Classification Thresholds',\n",
    "    val_metrics['bal_acc'],\n",
    "    'Balanced Accuracy',\n",
    "    disp_imp_err,\n",
    "    '1 - min(DI, 1 / DI)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825d9eba-683d-4826-b67f-3c6fecd496fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(\n",
    "    thresh_arr,\n",
    "    'Classification Thresholds',\n",
    "    val_metrics['bal_acc'],\n",
    "    'Balanced Accuracy',\n",
    "    val_metrics['avg_odds_diff'],\n",
    "    'avg. odds diff.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1051046-0f26-46eb-8908-48de07e8180f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_metrics(metrics, thresh_arr):\n",
    "    \"\"\"\n",
    "    Function to describe the metrics in the AIF360 framework\n",
    "    \"\"\"\n",
    "    # Get the index of the best balanced accuracy\n",
    "    best_ind = np.argmax(metrics['bal_acc'])\n",
    "\n",
    "    # Print the metric outputs\n",
    "    print(\n",
    "        \"Threshold corresponding to Best balanced accuracy: {:6.4f}\".format(thresh_arr[best_ind])\n",
    "    )\n",
    "    print(\n",
    "        \"Best balanced accuracy: {:6.4f}\".format(metrics['bal_acc'][best_ind])\n",
    "    )\n",
    "\n",
    "    # Compute the disparity impact\n",
    "    disp_imp_at_best_ind = (\n",
    "        1 - min(metrics['disp_imp'][best_ind], 1 / metrics['disp_imp'][best_ind])\n",
    "    )\n",
    "    print(\n",
    "        \"Corresponding 1-min(DI, 1/DI) value: {:6.4f}\".format(disp_imp_at_best_ind)\n",
    "    )\n",
    "    print(\n",
    "        \"Corresponding average odds difference value: {:6.4f}\".format(metrics['avg_odds_diff'][best_ind])\n",
    "    )\n",
    "    print(\n",
    "        \"Corresponding statistical parity difference value: {:6.4f}\".format(metrics['stat_par_diff'][best_ind])\n",
    "    )\n",
    "    print(\n",
    "        \"Corresponding equal opportunity difference value: {:6.4f}\".format(metrics['eq_opp_diff'][best_ind])\n",
    "    )\n",
    "    print(\n",
    "        \"Corresponding Theil index value: {:6.4f}\".format(metrics['theil_ind'][best_ind])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02136b9-8cd6-4f7a-861c-8f34d8435e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_metrics(val_metrics, thresh_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27c81e4-f70d-4043-bd66-63f96807c3ba",
   "metadata": {},
   "source": [
    "### Testing MLP model on original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a796f6-3189-4a1c-b3e9-0d42f2bf3263",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_orig_metrics = test(\n",
    "    dataset=dataset_orig_panel19_test,\n",
    "    model=mlp_orig_panel19,\n",
    "    thresh_arr=[thresh_arr[lr_orig_best_ind]]\n",
    ")\n",
    "describe_metrics(mlp_orig_metrics, [thresh_arr[mlp_orig_best_ind]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637a9ccc-6a1a-491b-a9c9-27fa8d8f5cab",
   "metadata": {},
   "source": [
    "# Bias mitigation using in-procesesing technique - Prejudice Remover (PR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba0d086-51f1-428b-98da-159e4980a78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the prejudice remover and standar scaler\n",
    "# & transform the data\n",
    "model = PrejudiceRemover(sensitive_attr=sens_attr, eta=25.0)\n",
    "pr_orig_scaler = StandardScaler()\n",
    "\n",
    "dataset = dataset_orig_panel19_train.copy()\n",
    "dataset.features = pr_orig_scaler.fit_transform(dataset.features)\n",
    "\n",
    "pr_orig_panel19 = model.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71a7024-884c-4b5d-b32e-75a2db35b2cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdl-survey",
   "language": "python",
   "name": "cdl-survey"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
