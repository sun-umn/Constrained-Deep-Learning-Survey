{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a4258ce-b448-4e6d-8b74-fe2ed91bed68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ryandevera/data-science/umn_environments/Constrained-Deep-Learning-Survey'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1956e81c-2397-40d8-b7ec-d6dfad14dbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-17 11:35:25.550929: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# stdlib\n",
    "import math\n",
    "import random\n",
    "\n",
    "# third party\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow_constrained_optimization as tfco\n",
    "import warnings\n",
    "\n",
    "# first party\n",
    "from cdlsurvey.data import get_data\n",
    "from cdlsurvey.models import Model\n",
    "\n",
    "# Disable eager execution\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "# suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For plotting in notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6eade18-286d-4de4-8419-6830c335443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROTECTED_COLUMNS = ['gender_Female', 'gender_Male', 'race_White', 'race_Black']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0181c8b-6878-4ff4-bcb7-eb73585692bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.  3.  0.  1. nan]\n",
      "[ 2.  3.  0.  1. nan]\n",
      "[ 2.  3.  0.  1. nan]\n",
      "[ 2.  3.  0.  1. nan]\n",
      "[ 2.  3.  0.  1. nan]\n"
     ]
    }
   ],
   "source": [
    "CATEGORICAL_COLUMNS = [\n",
    "    'workclass', 'education', 'marital_status', 'occupation', 'relationship',\n",
    "    'race', 'gender', 'native_country'\n",
    "]\n",
    "CONTINUOUS_COLUMNS = [\n",
    "    'age', 'capital_gain', 'capital_loss', 'hours_per_week', 'education_num'\n",
    "]\n",
    "COLUMNS = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
    "    'marital_status', 'occupation', 'relationship', 'race', 'gender',\n",
    "    'capital_gain', 'capital_loss', 'hours_per_week', 'native_country',\n",
    "    'income_bracket'\n",
    "]\n",
    "LABEL_COLUMN = 'label'\n",
    "\n",
    "PROTECTED_COLUMNS = [\n",
    "    'gender_Female', 'gender_Male', 'race_White', 'race_Black'\n",
    "]\n",
    "\n",
    "def get_data():\n",
    "    train_df_raw = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\", names=COLUMNS, skipinitialspace=True)\n",
    "    test_df_raw = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\", names=COLUMNS, skipinitialspace=True, skiprows=1)\n",
    "\n",
    "    train_df_raw[LABEL_COLUMN] = (train_df_raw['income_bracket'].apply(lambda x: '>50K' in x)).astype(int)\n",
    "    test_df_raw[LABEL_COLUMN] = (test_df_raw['income_bracket'].apply(lambda x: '>50K' in x)).astype(int)\n",
    "    # Preprocessing Features\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "    # Functions for preprocessing categorical and continuous columns.\n",
    "    def binarize_categorical_columns(input_train_df, input_test_df, categorical_columns=[]):\n",
    "\n",
    "        def fix_columns(input_train_df, input_test_df):\n",
    "            test_df_missing_cols = set(input_train_df.columns) - set(input_test_df.columns)\n",
    "            for c in test_df_missing_cols:\n",
    "                input_test_df[c] = 0\n",
    "                train_df_missing_cols = set(input_test_df.columns) - set(input_train_df.columns)\n",
    "            for c in train_df_missing_cols:\n",
    "                input_train_df[c] = 0\n",
    "                input_train_df = input_train_df[input_test_df.columns]\n",
    "            return input_train_df, input_test_df\n",
    "\n",
    "        # Binarize categorical columns.\n",
    "        binarized_train_df = pd.get_dummies(input_train_df, columns=categorical_columns)\n",
    "        binarized_test_df = pd.get_dummies(input_test_df, columns=categorical_columns)\n",
    "        # Make sure the train and test dataframes have the same binarized columns.\n",
    "        fixed_train_df, fixed_test_df = fix_columns(binarized_train_df, binarized_test_df)\n",
    "        return fixed_train_df, fixed_test_df\n",
    "\n",
    "    def bucketize_continuous_column(input_train_df,\n",
    "                                  input_test_df,\n",
    "                                  continuous_column_name,\n",
    "                                  num_quantiles=None,\n",
    "                                  bins=None):\n",
    "        assert (num_quantiles is None or bins is None)\n",
    "        if num_quantiles is not None:\n",
    "            train_quantized, bins_quantized = pd.qcut(\n",
    "              input_train_df[continuous_column_name],\n",
    "              num_quantiles,\n",
    "              retbins=True,\n",
    "              labels=False)\n",
    "            input_train_df[continuous_column_name] = pd.cut(\n",
    "              input_train_df[continuous_column_name], bins_quantized, labels=False)\n",
    "            input_test_df[continuous_column_name] = pd.cut(\n",
    "              input_test_df[continuous_column_name], bins_quantized, labels=False)\n",
    "        elif bins is not None:\n",
    "            input_train_df[continuous_column_name] = pd.cut(\n",
    "              input_train_df[continuous_column_name], bins, labels=False)\n",
    "            input_test_df[continuous_column_name] = pd.cut(\n",
    "              input_test_df[continuous_column_name], bins, labels=False)\n",
    "\n",
    "        print(input_train_df['age'].unique())\n",
    "\n",
    "    # Filter out all columns except the ones specified.\n",
    "    train_df = train_df_raw[CATEGORICAL_COLUMNS + CONTINUOUS_COLUMNS + [LABEL_COLUMN]]\n",
    "    test_df = test_df_raw[CATEGORICAL_COLUMNS + CONTINUOUS_COLUMNS + [LABEL_COLUMN]]\n",
    "    \n",
    "    # Bucketize continuous columns.\n",
    "    bucketize_continuous_column(train_df, test_df, 'age', num_quantiles=4)\n",
    "    bucketize_continuous_column(train_df, test_df, 'capital_gain', bins=[-1, 1, 4000, 10000, 100000])\n",
    "    bucketize_continuous_column(train_df, test_df, 'capital_loss', bins=[-1, 1, 1800, 1950, 4500])\n",
    "    bucketize_continuous_column(train_df, test_df, 'hours_per_week', bins=[0, 39, 41, 50, 100])\n",
    "    bucketize_continuous_column(train_df, test_df, 'education_num', bins=[0, 8, 9, 11, 16])\n",
    "    train_df, test_df = binarize_categorical_columns(train_df, test_df, categorical_columns=CATEGORICAL_COLUMNS + CONTINUOUS_COLUMNS)\n",
    "    feature_names = list(train_df.keys())\n",
    "    feature_names.remove(LABEL_COLUMN)\n",
    "    num_features = len(feature_names)\n",
    "    \n",
    "    return train_df, test_df, feature_names\n",
    "\n",
    "train_df, test_df, FEATURE_NAMES = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6367c64-5261-4b32-acf6-08f83e49cab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.  3.  0.  1. nan]\n",
      "[ 2.  3.  0.  1. nan]\n",
      "[ 2.  3.  0.  1. nan]\n",
      "[ 2.  3.  0.  1. nan]\n",
      "[ 2.  3.  0.  1. nan]\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df, FEATURE_NAMES = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9904b498-ac80-4683-87a6-3e3e2741a95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Model(object):\n",
    "#     def __init__(self,\n",
    "#                  tpr_max_diff=0):\n",
    "#         tf.random.set_random_seed(123)\n",
    "#         self.tpr_max_diff = tpr_max_diff\n",
    "#         num_features = len(FEATURE_NAMES)\n",
    "#         self.features_placeholder = tf.placeholder(\n",
    "#             tf.float32, shape=(None, num_features), name='features_placeholder')\n",
    "#         self.labels_placeholder = tf.placeholder(\n",
    "#             tf.float32, shape=(None, 1), name='labels_placeholder')\n",
    "#         self.protected_placeholders = [tf.placeholder(tf.float32, shape=(None, 1), name=attribute+\"_placeholder\") for attribute in PROTECTED_COLUMNS]\n",
    "#         # We use a linear model.\n",
    "#         self.predictions_tensor = tf.layers.dense(inputs=self.features_placeholder, units=1, activation=None)\n",
    "\n",
    "\n",
    "#     def build_train_op(self,\n",
    "#                        learning_rate,\n",
    "#                        unconstrained=False):\n",
    "#         ctx = tfco.rate_context(self.predictions_tensor, self.labels_placeholder)\n",
    "#         positive_slice = ctx.subset(self.labels_placeholder > 0) \n",
    "#         overall_tpr = tfco.positive_prediction_rate(positive_slice)\n",
    "#         constraints = []\n",
    "#         if not unconstrained:\n",
    "#             for placeholder in self.protected_placeholders:\n",
    "#                 slice_tpr = tfco.positive_prediction_rate(ctx.subset((placeholder > 0) & (self.labels_placeholder > 0)))\n",
    "#                 constraints.append(slice_tpr >= overall_tpr - self.tpr_max_diff)\n",
    "#         mp = tfco.RateMinimizationProblem(tfco.error_rate(ctx), constraints)\n",
    "#         opt = tfco.ProxyLagrangianOptimizerV1(tf.train.AdamOptimizer(learning_rate))\n",
    "#         self.train_op = opt.minimize(mp)\n",
    "\n",
    "#         return self.train_op\n",
    "  \n",
    "#     def feed_dict_helper(self, dataframe):\n",
    "#         feed_dict = {self.features_placeholder:\n",
    "#                   dataframe[FEATURE_NAMES],\n",
    "#               self.labels_placeholder:\n",
    "#                   dataframe[[LABEL_COLUMN]],}\n",
    "#         for i, protected_attribute in enumerate(PROTECTED_COLUMNS):\n",
    "#             feed_dict[self.protected_placeholders[i]] = dataframe[[protected_attribute]]\n",
    "\n",
    "#         import pdb; pdb.set_trace()\n",
    "#         return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e159c7c-4c35-4bb9-8aed-fc51cafc8b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_generator(model,\n",
    "                       train_df,\n",
    "                       test_df,\n",
    "                       minibatch_size,\n",
    "                       num_iterations_per_loop=1,\n",
    "                       num_loops=1):\n",
    "    random.seed(31337)\n",
    "    num_rows = train_df.shape[0]\n",
    "    minibatch_size = min(minibatch_size, num_rows)\n",
    "    permutation = list(range(train_df.shape[0]))\n",
    "    random.shuffle(permutation)\n",
    "\n",
    "    session = tf.Session()\n",
    "    session.run((tf.global_variables_initializer(),\n",
    "               tf.local_variables_initializer()))\n",
    "\n",
    "    minibatch_start_index = 0\n",
    "    for n in range(num_loops):\n",
    "        for _ in range(num_iterations_per_loop):\n",
    "            minibatch_indices = []\n",
    "            while len(minibatch_indices) < minibatch_size:\n",
    "                minibatch_end_index = (\n",
    "                minibatch_start_index + minibatch_size - len(minibatch_indices))\n",
    "                if minibatch_end_index >= num_rows:\n",
    "                    minibatch_indices += range(minibatch_start_index, num_rows)\n",
    "                    minibatch_start_index = 0\n",
    "                else:\n",
    "                    minibatch_indices += range(minibatch_start_index, minibatch_end_index)\n",
    "                    minibatch_start_index = minibatch_end_index\n",
    "                    \n",
    "            session.run(\n",
    "                  model.train_op,\n",
    "                  feed_dict=model.feed_dict_helper(\n",
    "                      train_df.iloc[[permutation[ii] for ii in minibatch_indices]]))\n",
    "\n",
    "        train_predictions = session.run(\n",
    "            model.predictions_tensor,\n",
    "            feed_dict=model.feed_dict_helper(train_df))\n",
    "        test_predictions = session.run(\n",
    "            model.predictions_tensor,\n",
    "            feed_dict=model.feed_dict_helper(test_df))\n",
    "\n",
    "        yield (train_predictions, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "673ef15c-7f52-4d8a-bf76-8b402e490fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_rate(predictions, labels):\n",
    "    signed_labels = (\n",
    "      (labels > 0).astype(np.float32) - (labels <= 0).astype(np.float32))\n",
    "    numerator = (np.multiply(signed_labels.values, predictions.values) <= 0).sum()\n",
    "    denominator = predictions.shape[0]\n",
    "    return float(numerator) / float(denominator)\n",
    "\n",
    "\n",
    "def positive_prediction_rate(predictions, subset):\n",
    "    numerator = np.multiply((predictions > 0).astype(np.float32),\n",
    "                          (subset > 0).astype(np.float32)).sum()\n",
    "    denominator = (subset > 0).sum()\n",
    "    return float(numerator) / float(denominator)\n",
    "\n",
    "def tpr(df):\n",
    "    \"\"\"Measure the true positive rate.\"\"\"\n",
    "    fp = sum((df['predictions'] >= 0.0) & (df[LABEL_COLUMN] > 0.5))\n",
    "    ln = sum(df[LABEL_COLUMN] > 0.5)\n",
    "    return float(fp) / float(ln)\n",
    "\n",
    "def _get_error_rate_and_constraints(df, tpr_max_diff):\n",
    "    \"\"\"Computes the error and fairness violations.\"\"\"\n",
    "    error_rate_local = error_rate(df[['predictions']], df[[LABEL_COLUMN]])\n",
    "    overall_tpr = tpr(df)\n",
    "    return error_rate_local, [(overall_tpr - tpr_max_diff) - tpr(df[df[protected_attribute] > 0.5]) for protected_attribute in PROTECTED_COLUMNS]\n",
    "\n",
    "def _get_exp_error_rate_constraints(cand_dist, error_rates_vector, constraints_matrix):\n",
    "    \"\"\"Computes the expected error and fairness violations on a randomized solution.\"\"\"\n",
    "    expected_error_rate = np.dot(cand_dist, error_rates_vector)\n",
    "    expected_constraints = np.matmul(cand_dist, constraints_matrix)\n",
    "    return expected_error_rate, expected_constraints\n",
    "\n",
    "def training_helper(model,\n",
    "                    train_df,\n",
    "                    test_df,\n",
    "                    minibatch_size,\n",
    "                    num_iterations_per_loop=1,\n",
    "                    num_loops=1):\n",
    "    train_error_rate_vector = []\n",
    "    train_constraints_matrix = []\n",
    "    test_error_rate_vector = []\n",
    "    test_constraints_matrix = []\n",
    "    for train, test in training_generator(\n",
    "      model, train_df, test_df, minibatch_size, num_iterations_per_loop,\n",
    "      num_loops):\n",
    "        train_df['predictions'] = train\n",
    "        test_df['predictions'] = test\n",
    "\n",
    "        train_error_rate, train_constraints = _get_error_rate_and_constraints(\n",
    "          train_df, model.tpr_max_diff)\n",
    "        train_error_rate_vector.append(train_error_rate)\n",
    "        train_constraints_matrix.append(train_constraints)\n",
    "\n",
    "        test_error_rate, test_constraints = _get_error_rate_and_constraints(\n",
    "            test_df, model.tpr_max_diff)\n",
    "        test_error_rate_vector.append(test_error_rate)\n",
    "        test_constraints_matrix.append(test_constraints)\n",
    "\n",
    "    return (train_error_rate_vector, train_constraints_matrix, test_error_rate_vector, test_constraints_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62c6ff35-0de3-4163-afb7-aa4a525d494d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(\n",
    "    tpr_max_diff=0.05,\n",
    "    protected_columns=PROTECTED_COLUMNS,\n",
    "    feature_names=FEATURE_NAMES,\n",
    "    label_column=LABEL_COLUMN,\n",
    ")\n",
    "model.build_train_op(0.01, unconstrained=True)\n",
    "\n",
    "# training_helper returns the list of errors and violations over each epoch.\n",
    "train_errors, train_violations, test_errors, test_violations = training_helper(\n",
    "      model,\n",
    "      train_df,\n",
    "      test_df,\n",
    "      100,\n",
    "      num_iterations_per_loop=326,\n",
    "      num_loops=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e7a518c-fe36-4a6b-9136-9f0c49f5d529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error 0.14237891956635237\n",
      "Train Violation 0.04671353000944656\n",
      "\n",
      "Test Error 0.14323444505865732\n",
      "Test Violation 0.053536431960071684\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Error\", train_errors[-1])\n",
    "print(\"Train Violation\", max(train_violations[-1]))\n",
    "print()\n",
    "print(\"Test Error\", test_errors[-1])\n",
    "print(\"Test Violation\", max(test_violations[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054ae740-2fa0-41b0-88d9-2f1d2280203d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(tpr_max_diff=0.05)\n",
    "model.build_train_op(0.01, unconstrained=False)\n",
    "\n",
    "# training_helper returns the list of errors and violations over each epoch.\n",
    "train_errors, train_violations, test_errors, test_violations = training_helper(\n",
    "      model,\n",
    "      train_df,\n",
    "      test_df,\n",
    "      100,\n",
    "      num_iterations_per_loop=326,\n",
    "      num_loops=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e55c114-91e1-4639-a4fd-d1843574b7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train Error\", train_errors[-1])\n",
    "print(\"Train Violation\", max(train_violations[-1]))\n",
    "print()\n",
    "print(\"Test Error\", test_errors[-1])\n",
    "print(\"Test Violation\", max(test_violations[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7689ccc-aaa1-41f2-abea-6849b04512a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train Error\", np.mean(train_errors))\n",
    "print(\"Train Violation\", max(np.mean(train_violations, axis=0)))\n",
    "print()\n",
    "print(\"Test Error\", np.mean(test_errors))\n",
    "print(\"Test Violation\", max(np.mean(test_violations, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8777d432-237f-4a46-a7c5-01e1aa162a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cand_dist = tfco.find_best_candidate_distribution(train_errors, train_violations)\n",
    "print(cand_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974a0c3a-8322-49ac-9a97-624f69f4f333",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_stoch_error_train, m_stoch_violations_train = _get_exp_error_rate_constraints(cand_dist, train_errors, train_violations)\n",
    "m_stoch_error_test, m_stoch_violations_test = _get_exp_error_rate_constraints(cand_dist, test_errors, test_violations)\n",
    "\n",
    "print(\"Train Error\", m_stoch_error_train)\n",
    "print(\"Train Violation\", max(m_stoch_violations_train))\n",
    "print()\n",
    "print(\"Test Error\", m_stoch_error_test)\n",
    "print(\"Test Violation\", max(m_stoch_violations_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "373c9071-50a8-45fb-8eb9-151144426842",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b927fd71-6717-493c-9581-960bd50b99ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b770a83-599a-4a00-8363-e9ef9105a4b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdl-survey",
   "language": "python",
   "name": "cdl-survey"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
